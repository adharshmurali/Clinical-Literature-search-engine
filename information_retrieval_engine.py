# -*- coding: utf-8 -*-
"""Information retrieval engine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XBto4FhyFYtgxb0FeoZ2XE0tmSdeM3gN
"""

#Importing the libraries for building the retrieval engine
import whoosh as wp
import pandas as pd
import os
import re
import xml.etree.ElementTree as ET

from whoosh import index
from whoosh.index import open_dir
from whoosh.fields import Schema, ID, TEXT
from whoosh.qparser import QueryParser, MultifieldParser
from whoosh.query import *

#Searching the indexed documents.
ix = open_dir("./indexdir1/")
query_b = QueryParser('content', ix.schema).parse('cdk4')
with ix.searcher() as srch:
    res_b = srch.search(query_b, limit=10)
    for i in res_b:
        print(i['name'])

ix = open_dir("./indexdir1/")
query_b = QueryParser('name', ix.schema).parse('NCT01692496')
with ix.searcher() as srch:
    res_b = srch.search(query_b, limit=10)
    for i in res_b:
        print(i['name'])

mparser = MultifieldParser(["Title","content"], schema=schema)

mparser.add_plugin(wp.qparser.FuzzyTermPlugin)

ix = open_dir("./indexdir1/")
q = mparser.parse('"Colon" "cancer" "BRAF" "V600E"')
out = []
with ix.searcher() as srch:
    res_b = srch.search(q, limit=100)
    for i in res_b:
        s = str(i['name'])[-11:]
        out.append(s)
out

eg = open('./topic_19.txt','w')

pos = 1
score = 10
for i in out:
    eg.write('2\tQ0\t{}\t{}\t{}\trun'.format(i, str(pos), str(score)))
    eg.write('\n')
    pos+=1
    score -=0.01
eg.close()



query_iter[1]

q = mparser.parse('"Colon" "cancer" "BRAF" "V600E"')
with ix.searcher() as srch:
    res_b = srch.search(q, limit=100)
    print(len(res_b))

search = ix.searcher()

res =search.search(q, limit = 100)

len(res)

search.close()

def retrieve_num(query_list):
    queries = []
    srch = ix.searcher()
    for alist in query_list:
        q = " ".join(alist)
        query = mparser.parse(q)
        res = srch.search(query, limit=100)
        if len(res)!=0:
            queries.append((q,len(res)))
        
    srch.close()
    
    return queries

l = retrieve_num(query_iter[1]);l

sorted(l, key=lambda x: x[-1])

def get_final_queries(iters):
    topic_queries = {}
    for num, t in iters.items():
        queries = retrieve_num(t)
        topic_queries[num] = sorted(queries, key=lambda x: x[-1])
        
    return topic_queries

q = get_final_queries(query_iter)
q

q[1]

from collections import OrderedDict

def search_atopic(query_list):
    search = ix.searcher()
    result = []
    
    for q in query_list:
        query = mparser.parse(q[0])
        res = search.search(query, limit=100)
        for i in res:
            s = str(i['name'])[-11:]
            result.append(s)
            
    search.close()
    result = list(OrderedDict.fromkeys(result)) 
    
    return result

search_atopic(q[1])

r_all = {}
for i in range(1,31):
    r_all[i] = search_atopic(q[i])

r_all

out = open('./whoosh.txt','w')
for num, id_list in r_all.items():
    pos = 1
    score = 10
    for nctid in id_list[:500]:
        out.write('{}\tQ0\t{}\t{}\t{}\trun'.format(num, nctid, str(pos), str(score)))
        out.write('\n')
        pos+=1
        score -=0.05
                
out.close()

#tokenizer
my_analyzer= wp.analysis.RegexTokenizer() | wp.analysis.LowercaseFilter() | wp.analysis.StopFilter()| wp.analysis.StemFilter()

[token.text for token in my_analyzer(u("This is a dose-escalation study of the CDK4/6 inhibitor liposarcomas"))]

[token.text for token in my_analyzer(u('35 Years'))]

from nltk.stem import WordNetLemmatizer

new_analyzer = wp.analysis.RegexTokenizer() | wp.analysis.LowercaseFilter() | wp.analysis.StopFilter()| wp.analysis.StemFilter()

if not os.path.exists("./index"):
    os.mkdir("./index")
#Creating the schema for the index as ID , content being the concatenated string of both the detailed and brief summary together
#Gender #Min AGe and #Max age
#If in case the Min Age is N/A make it 0 if Max Age is N/A make it 1
schema =  Schema(name=ID(stored=True, analyzer=wp.analysis.RegexTokenizer()), Title=TEXT(stored=True, analyzer=new_analyzer, field_boost=1.5),content=TEXT(stored=True,analyzer=new_analyzer))
ix = index.create_in("./index", schema)
writer = ix.writer()

writer.add_document(name='https://clinicaltrials.gov/show/NCT02429089',
                    content="This is a dose-escalation study of the CDK4/6 inhibitor ribociclib in combination with\n      standard-dose doxorubicin.\n\n      PRIMARY OBJECTIVES:\n\n      I. To determine the recommended phase 2 dose (RP2D) of ribociclib in combination with\n      doxorubicin in subjects with advanced soft tissue sarcomas.\n\n      SECONDARY OBJECTIVES:\n\n      I. To assess preliminary anti-tumor activity of ribociclib in combination with doxorubicin in\n      subjects with advanced soft tissue sarcomas (Progression-Free Survival and Overall Response\n      Rate).\n\n      II. To characterize the safety and tolerability of ribociclib in combination with\n      doxorubicin.\n\n      A mandatory biopsy will be obtained after 7 days of ribociclib treatment.\n\n      TREATMENT: Patients receive ribociclib orally (PO) daily on days 1-7, and doxorubicin\n      intravenously (IV) on day 10. Treatment repeats every 21 days for up to 6 courses in the\n      absence of disease progression or unacceptable toxicity. After 6 courses, patients may\n      receive maintenance treatment with ribociclib PO daily on days 1-21. Courses repeat every 28\n      days in the absence of disease progression or unacceptable toxicity.\n\n      STARTING DOSE COHORT: Ribociclib 400 mg with doxorubicin 75 mg/m2.\n\n      FOLLOW-UP: After completion of study treatment, patients are followed up at 30 days and then\n      every 12 weeks for 12 months.\n\n      RATIONALE: Over-expression of CDK4 or loss of the CDK4 inhibitor p16 are common in sarcomas\n      and result in a selective growth advantage by bypassing normal cell cycle checkpoints. Intact\n      pRb is required for CDK4/6 inhibition to be effective, therefore all eligible subjects must\n      have documented pRb expression by IHC on archival tissue. Synergy between CDK4 inhibition and\n      chemotherapy has been documented in preclinical models when given sequentially, suggestion a\n      role for cell cycle synchronization.",
                    Title="dose-escalation study of the CDK4/6 and LIPOSARCOMA")
writer.commit()

ix = open_dir("./index/")
query_b = QueryParser('content', ix.schema).parse('sarcoma cdk4')
with ix.searcher() as srch:
    res_b = srch.search(query_b, limit=10)
    for i in res_b:
        print(i['content'])

#Creating a index file in whoosh
#Initially creating a schema to organize the data for the time being the only ID and content
if not os.path.exists("./indexdir1"):
    os.mkdir("./indexdir1")
#Creating the schema for the index as ID , content being the concatenated string of both the detailed and brief summary together
#Gender #Min AGe and #Max age
#If in case the Min Age is N/A make it 0 if Max Age is N/A make it 1
schema =  Schema(name=ID(stored=True, analyzer=wp.analysis.RegexTokenizer()), Title=TEXT(stored=True, analyzer=my_analyzer, field_boost=1.5),content=TEXT(stored=True,analyzer=my_analyzer),gender=TEXT(stored=True),minage=TEXT(stored=True),maxage=TEXT(stored=True))
ix = index.create_in("./indexdir1", schema)
writer = ix.writer()

#traversing through all the files in the directory
for i in [a for a in os.listdir("./clinicaltrials_xml/") if a.startswith('0')]:
    for j in [b for b in os.listdir("./clinicaltrials_xml/"+i) if b.startswith('0')]:
        for k in os.listdir("./clinicaltrials_xml/"+i+"/"+j+"/"):
            root=ET.parse("./clinicaltrials_xml/"+i+'/'+j+'/'+k).getroot()
    #Got the root element now it is time to get into the textbloc of brief_summary and the detail_description
            try:
                content_text=(root.find('./brief_summary/textblock').text)+(root.find('./detailed_description/textblock').text)
            except:
                if root.find('./detailed_description/textblock') is None and root.find('./detailed_description/textblock') is None:
                    content_text=' '
                elif root.find('./brief_summary/textblock') is None:
                    content_text=root.find('./detailed_description/textblock').text
                else:
                    content_text=root.find('./brief_summary/textblock').text
                    
            try:
                title = (root.find('./brief_title').text)+(root.find('./official_title').text)
            except:
                if root.find('./brief_title') is None and root.find('./official_title') is None:
                    title = " "
                elif root.find('./brief_title') is None:
                    title = root.find('./official_title').text
                else:
                    title = root.find('./brief_title').text

            try:
                url=root.find('./required_header/url').text
            except:
                url=" "
            try:
                if root.find('./eligibility/minimum_age').text=='N/A':
                    miage=u'0'
                else:
                    miage=root.find('./eligibility/minimum_age').text
            except:
                miage=u'0'
            try:
                if root.find('./eligibility/maximum_age').text=='N/A':
                    maage=u'200'
                else:
                    maage=root.find('./eligibility/maximum_age').text
            except:
                maage=u'200'
            try:
                ge=root.find('./eligibility/gender').text
            except:
                ge=' '
    #Schema(name=ID(stored=True), Title=TEXT(stored=True),content=TEXT(stored=True),gender=TEXT(stored=True),minage=TEXT(stored=True),maxage=TEXT(stored=True))
            writer.add_document(name=url,content=content_text,Title=title,gender=ge,minage=miage,maxage=maage)
writer.commit()
    #Now extracting the



def parse_topics(filename):
    topic_dict ={}
    
    e = ET.parse(filename)
    for topic in e.findall('topic'):
        topic_id = topic.get("number")
        topic_dict[int(topic_id)] = {}
        
        if topic.find("disease") is not None:
            disease_query = topic.find('disease').text
            topic_dict[int(topic_id)]['disease'] = disease_query
            
        if topic.find('gene') is not None:
            gene_query = topic.find('gene').text
            topic_dict[int(topic_id)]['gene'] = gene_query
            
    return topic_dict

topics2017 = parse_topics("topics2017.xml")
topics2017

topics_all = get_query_terms(topics2017)
topics_all

l = '"Colorectal" "cancer" "FGFR1" "Amplification"'.split(' ')
l

query_iter = query_iteration(topics_all)
query_iter

import itertools

for i in itertools.combinations(l,len(l)):
    print(' '.join(i))



def query_iteration(topics):
    query_dict = {}
    
    for topic_number , query in topics.items():
        query_dict[topic_number] = []
        term_list = query.split(' ')
        
        for i in itertools.combinations(term_list,len(term_list)):
            query_dict[topic_number].append(i)
        
        if len(term_list)>5:
            for i in itertools.combinations(term_list,len(term_list)-3):
                query_dict[topic_number].append(i)
            for i in itertools.combinations(term_list,len(term_list)-2):
                query_dict[topic_number].append(i)
            for i in itertools.combinations(term_list,len(term_list)-1):
                query_dict[topic_number].append(i)
        elif len(term_list)>3 and len(term_list)<5:
            for i in itertools.combinations(term_list,len(term_list)-2):
                query_dict[topic_number].append(i)
            for i in itertools.combinations(term_list,len(term_list)-1):
                query_dict[topic_number].append(i)
        else:
            for i in itertools.combinations(term_list,len(term_list)-1):
                query_dict[topic_number].append(i)

        
    return query_dict

def get_query_terms(topics, expansion_type="term", boosting=False):
    
    term_dict = {}
    
    boost_term = ""
    if boosting == True:
        boost_term = "treat patient"
    

    
    if expansion_type is None:
        expansion_type = "term"
        
    if "term" in expansion_type:
        for topic_number, topic_dict in topics.items():
            
            if " " not in topic_dict['disease'].strip():
                processed_disease = '''"{}" '''.format(topic_dict['disease'])
            else:
                processed_disease = ""
                processed_disease += '''"{}" '''.format(topic_dict['disease'].strip().split(" ")[0]) 
                for substr in topic_dict['disease'].strip().split(" ")[1:]:
                    processed_disease += '''"{}" '''.format(substr)           
            
            
            processed_gene = ""
            processed_variant = ""
            
            if "," not in topic_dict["gene"]:
                if " " not in topic_dict["gene"].strip():
                    processed_gene = '''"{}" '''.format(topic_dict['gene'].strip())
                elif "(" in topic_dict["gene"]:
                    preprocessed_gene = topic_dict["gene"].replace("(","").replace(")","")
                    preprocessed_gene_array = preprocessed_gene.split(" ")
                    processed_gene = '''"{}" '''.format(preprocessed_gene_array[0])
                    processed_variant = '''"{}" '''.format(preprocessed_gene_array[1])
                elif "-" in topic_dict["gene"]:
                    preprocessed_gene_array = topic_dict["gene"].split(" ")
                    for single in preprocessed_gene_array[0].split("-"):
                        processed_gene += '''"{}" '''.format(single.strip())
                    processed_variant += '''"{}" '''.format(preprocessed_gene_array[1])
                else:
                    preprocessed_gene_array = topic_dict["gene"].split(" ")
                    processed_gene = '''"{}" '''.format(preprocessed_gene_array[0])
                    processed_variant = '''"{}" '''.format(preprocessed_gene_array[1])
            
            else:
                for single in topic_dict["gene"].split(","):
                    if " " not in single.strip():
                        processed_gene += '''"{}" '''.format(single.strip())
                    elif "(" in single:
                        preprocessed_gene = single.replace("(","").replace(")","").strip(" ")
                        preprocessed_gene_array = preprocessed_gene.split(" ")
                        processed_gene += '''"{}" '''.format(preprocessed_gene_array[0])
                        processed_variant += '''"{}" '''.format(preprocessed_gene_array[1])
                    else:
                        preprocessed_gene_array = single.strip().split(" ")
                        processed_gene += '''"{}" '''.format(preprocessed_gene_array[0])
                        processed_variant += '''"{}" '''.format(preprocessed_gene_array[1])        
                                        
            term_string = processed_disease + processed_gene + processed_variant 
            term_dict[topic_number] = term_string.strip()
                        
 
                
    return term_dict

